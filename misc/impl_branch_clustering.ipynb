{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79922468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "\n",
    "from data_yours import ALS_50K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0627040",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a2891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_kmeans import KMeans, SoftKMeans, ConstrainedKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7a12f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48180"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ALS_50K()\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8afa2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample \n",
    "ds = ALS_50K(num_points=2048)\n",
    "sample = ds[25542]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32400bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold(rdpercents): \n",
    "    return torch.quantile(rdpercents, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fefda7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get xyz \n",
    "sample = sample[0]\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5de9b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a53bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor(sample, dtype=torch.float32).to(device)  # add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303077ae",
   "metadata": {},
   "source": [
    "## For a single point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bb654",
   "metadata": {},
   "source": [
    "## kNN Clustering function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae46c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans_pytorch import kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0369fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree_center_xy(pc): \n",
    "    return torch.mean(pc[:,:2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb7b0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(pc, c_idx):\n",
    "    \"\"\"\n",
    "    Splits the point cloud (pc) into clusters according to c_idx.\n",
    "    Returns a list of tensors, one per cluster.\n",
    "    \"\"\"\n",
    "    return [pc[c_idx == i] for i in range(c_idx.max() + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f2f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_clusters(pc, device, k=100): \n",
    "    c_idx, c_centroids = kmeans(\n",
    "        X=pc,\n",
    "        num_clusters=k,\n",
    "        distance='euclidean',\n",
    "        device=device\n",
    "    )\n",
    "    clusters = get_clusters(pc, c_idx)\n",
    "\n",
    "    clusters_dict = {\n",
    "        i: {\n",
    "            'centroid': c_centroids[i].to(device),\n",
    "            'points': clusters[i].to(device)\n",
    "        }\n",
    "        for i in range(len(c_centroids))\n",
    "    }\n",
    "    return clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63028ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(points):\n",
    "    mean = points.mean(dim=0)\n",
    "    X = points - mean\n",
    "    eigvals, eigvecs = torch.linalg.eigh(torch.cov(X.T))\n",
    "    idx = torch.argsort(eigvals, descending=True)\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    direction = eigvecs[:, 0]  # 1st principal component\n",
    "    normal = eigvecs[:, 1]     # 2nd principal component\n",
    "    return mean, direction, normal\n",
    "#compute_pca(clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be1d83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_residuals(cluster):\n",
    "\n",
    "\n",
    "    vec = cluster['points'] - cluster['centroid'] \n",
    "    normal = cluster['normal'] / torch.norm(cluster['normal']) \n",
    "    distances = torch.abs(torch.matmul(vec, normal))\n",
    "\n",
    "    return torch.sum(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c001fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters(c1, c2):\n",
    "    merged = torch.cat((c1['points'], c2['points']))\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60760dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_residual_diff_percent(c1, c2):\n",
    "    r_prior = c1['residual'] + c2['residual']\n",
    "    merged = {\n",
    "        'points': merge_clusters(c1, c2),\n",
    "    }\n",
    "\n",
    "    mean, direction, normal = compute_pca(merged['points'])\n",
    "    merged['centroid'] = mean\n",
    "    merged['direction'] = direction\n",
    "    merged['normal'] = normal\n",
    "    r_post = cluster_residuals(merged)\n",
    "    return (r_post - r_prior)/ r_prior * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31ce0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_pairs(centroids): \n",
    "    dmatrix = torch.cdist(centroids, centroids) \n",
    "    dmatrix = dmatrix.fill_diagonal_(float('inf'))\n",
    "    tril_indices = torch.tril_indices(dmatrix.size(0), dmatrix.size(1), offset=-1)\n",
    "    tril_indices = tril_indices.to(centroids.device)\n",
    "    distances = dmatrix[tril_indices[0], tril_indices[1]]\n",
    "    sorted_indices = distances.argsort()\n",
    "    sorted_indices = sorted_indices.to(centroids.device)\n",
    "    sorted_pairs = torch.stack((tril_indices[0][sorted_indices], tril_indices[1][sorted_indices]), dim=1)\n",
    "    sorted_distances = distances[sorted_indices]\n",
    "    return sorted_pairs, sorted_distances\n",
    "\n",
    "#distance_pairs(cl_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d656137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairs_angle(c1, c2): \n",
    "    angle = torch.acos(torch.dot(c1['normal'], c2['normal']) / (torch.norm(c1['normal']) * torch.norm(c2['normal'])))\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc36932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pairs_distance_angle(pairs, distances, angles):\n",
    "    # Sort by distance + small weight on angle\n",
    "    sort_metric = distances + 1e-2 * angles\n",
    "    sorted_indices = torch.argsort(sort_metric)\n",
    "    sorted_pairs = pairs[sorted_indices]\n",
    "    return sorted_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54e1dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold(rdpercents): \n",
    "    return torch.quantile(rdpercents, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22ab03d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_clusters(clusters, idx1, idx2):\n",
    "    # Merge two clusters and update the dictionary\n",
    "    c1 = clusters[idx1]\n",
    "    c2 = clusters[idx2]\n",
    "\n",
    "    merged_points = merge_clusters(c1, c2)\n",
    "    mean, direction, normal = compute_pca(merged_points)\n",
    "\n",
    "    new_cluster = {\n",
    "        'centroid': mean,\n",
    "        'direction': direction,\n",
    "        'normal': normal,\n",
    "        'points': merged_points,\n",
    "        'residual': cluster_residuals({'points': merged_points, 'centroid': mean, 'normal': normal})\n",
    "    }\n",
    "\n",
    "    for idx in sorted([idx1, idx2], reverse=True):\n",
    "        del clusters[idx]\n",
    "\n",
    "    # Add the new cluster with a temporary key\n",
    "    clusters[max(clusters.keys(), default=-1) + 1] = new_cluster\n",
    "\n",
    "    # Reindex clusters keys to be 0...len(clusters)-1\n",
    "    clusters = {new_idx: clusters[old_idx] for new_idx, old_idx in enumerate(sorted(clusters.keys()))}\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1988717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8019b93",
   "metadata": {},
   "source": [
    "## Iterative merging process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "74df2b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 12it [00:00, 19.21it/s, center_shift=0.000084, iteration=12, tol=0.000100]\n"
     ]
    }
   ],
   "source": [
    "# initialize clusters \n",
    "clusters = initialize_clusters(sample, device, k=100)\n",
    "centroids = torch.stack([clusters[i]['centroid'] for i in range(len(clusters))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d1581970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 clusters with < 3 points\n"
     ]
    }
   ],
   "source": [
    "# get clusters with shape < 3 \n",
    "clusters_to_merge = [k for k, v in clusters.items() if v['points'].shape[0] < 3]\n",
    "print(len(clusters_to_merge), 'clusters with < 3 points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "dd438b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pairs = distance_pairs(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4614284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(clusters_to_merge) > 0:\n",
    "    for k in clusters_to_merge: \n",
    "        mergin_candidate = [i for i in init_pairs[0] if i in init_pairs[0]][0]\n",
    "\n",
    "        clusters = update_clusters(clusters, mergin_candidate[0].item(), mergin_candidate[1].item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d0ce2141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "095c9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pca for each cluster \n",
    "for k, v in clusters.items():\n",
    "    mean, direction, normal = compute_pca(v['points'])\n",
    "    v['centroid'] = mean\n",
    "    v['direction'] = direction\n",
    "    v['normal'] = normal\n",
    "    v['residual'] = cluster_residuals(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "18f1f1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clusters)): \n",
    "    clusters[i]['residual'] = cluster_residuals(clusters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cfc738dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = torch.stack([clusters[i]['centroid'] for i in range(len(clusters))])\n",
    "init_pairs = distance_pairs(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "793c2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute angles between all pairs of clusters \n",
    "angles = [\n",
    "    pairs_angle(clusters[i.item()], clusters[j.item()])\n",
    "    for i, j in init_pairs[0]\n",
    "]\n",
    "\n",
    "angles = torch.tensor(angles, device=device)\n",
    "\n",
    "sorted_pairs = sort_pairs_distance_angle(init_pairs[0], init_pairs[1], angles)\n",
    "\n",
    "merged_resid = [\n",
    "    merged_residual_diff_percent(clusters[i], clusters[j])\n",
    "    for i, j in sorted_pairs.tolist()\n",
    "]\n",
    "\n",
    "\n",
    "t = compute_threshold(torch.tensor(merged_resid, device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c181f7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(58.6180, device='cuda:0')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c78bcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "88b71437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2a1bafd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for each step:\n",
      " Recompute centroids: 0.0046s,  Get pairs: 1750758688.4683s,  Compute angles: 1.9556s,  Compute merged residuals: 1750758696.2158s,  Find candidates: 2.0050s,  Find best pair: 1750758696.2160s,  Merge clusters: 2.0067s\n",
      "Merged clusters 64 and 19, 99 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0007s,  Get pairs: 1750758698.2237s,  Compute angles: 1.8701s,  Compute merged residuals: 1750758707.4071s,  Find candidates: 1.9159s,  Find best pair: 1750758707.4072s,  Merge clusters: 1.9169s\n",
      "Merged clusters 54 and 16, 98 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0007s,  Get pairs: 1750758709.3250s,  Compute angles: 1.8480s,  Compute merged residuals: 1750758718.1239s,  Find candidates: 1.9034s,  Find best pair: 1750758718.1240s,  Merge clusters: 1.9045s\n",
      "Merged clusters 66 and 41, 97 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0002s,  Get pairs: 1750758720.0313s,  Compute angles: 1.6105s,  Compute merged residuals: 1750758728.9430s,  Find candidates: 1.6530s,  Find best pair: 1750758728.9430s,  Merge clusters: 1.6543s\n",
      "Merged clusters 68 and 44, 96 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0006s,  Get pairs: 1750758730.5982s,  Compute angles: 1.8726s,  Compute merged residuals: 1750758739.0465s,  Find candidates: 1.9189s,  Find best pair: 1750758739.0465s,  Merge clusters: 1.9198s\n",
      "Merged clusters 64 and 44, 95 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0006s,  Get pairs: 1750758740.9672s,  Compute angles: 2.1135s,  Compute merged residuals: 1750758748.9389s,  Find candidates: 2.1673s,  Find best pair: 1750758748.9413s,  Merge clusters: 2.1752s\n",
      "Merged clusters 49 and 10, 94 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0003s,  Get pairs: 1750758751.1182s,  Compute angles: 1.7732s,  Compute merged residuals: 1750758758.7531s,  Find candidates: 1.8160s,  Find best pair: 1750758758.7534s,  Merge clusters: 1.8182s\n",
      "Merged clusters 47 and 14, 93 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0005s,  Get pairs: 1750758760.5728s,  Compute angles: 1.8033s,  Compute merged residuals: 1750758767.5389s,  Find candidates: 1.8539s,  Find best pair: 1750758767.5389s,  Merge clusters: 1.8557s\n",
      "Merged clusters 13 and 12, 92 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0010s,  Get pairs: 1750758769.3958s,  Compute angles: 1.6377s,  Compute merged residuals: 1750758776.3281s,  Find candidates: 1.6850s,  Find best pair: 1750758776.3282s,  Merge clusters: 1.6868s\n",
      "Merged clusters 28 and 9, 91 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0002s,  Get pairs: 1750758778.0160s,  Compute angles: 1.5163s,  Compute merged residuals: 1750758785.0770s,  Find candidates: 1.5544s,  Find best pair: 1750758785.0773s,  Merge clusters: 1.5564s\n",
      "Merged clusters 19 and 8, 90 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0003s,  Get pairs: 1750758786.6345s,  Compute angles: 1.4108s,  Compute merged residuals: 1750758793.2042s,  Find candidates: 1.4592s,  Find best pair: 1750758793.2044s,  Merge clusters: 1.4611s\n",
      "Merged clusters 34 and 7, 89 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0005s,  Get pairs: 1750758794.6667s,  Compute angles: 1.4824s,  Compute merged residuals: 1750758801.3776s,  Find candidates: 1.5234s,  Find best pair: 1750758801.3779s,  Merge clusters: 1.5259s\n",
      "Merged clusters 36 and 28, 88 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0005s,  Get pairs: 1750758802.9048s,  Compute angles: 1.5634s,  Compute merged residuals: 1750758809.5206s,  Find candidates: 1.6341s,  Find best pair: 1750758809.5229s,  Merge clusters: 1.6421s\n",
      "Merged clusters 49 and 13, 87 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0004s,  Get pairs: 1750758811.1664s,  Compute angles: 1.3604s,  Compute merged residuals: 1750758818.9265s,  Find candidates: 1.4085s,  Find best pair: 1750758818.9267s,  Merge clusters: 1.4104s\n",
      "Merged clusters 58 and 28, 86 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0004s,  Get pairs: 1750758820.3383s,  Compute angles: 1.7303s,  Compute merged residuals: 1750758827.4023s,  Find candidates: 1.7741s,  Find best pair: 1750758827.4024s,  Merge clusters: 1.7758s\n",
      "Merged clusters 45 and 3, 85 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0029s,  Get pairs: 1750758829.1794s,  Compute angles: 1.3932s,  Compute merged residuals: 1750758837.0463s,  Find candidates: 1.4457s,  Find best pair: 1750758837.0464s,  Merge clusters: 1.4478s\n",
      "Merged clusters 49 and 2, 84 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0002s,  Get pairs: 1750758838.4953s,  Compute angles: 1.4048s,  Compute merged residuals: 1750758845.4816s,  Find candidates: 1.4422s,  Find best pair: 1750758845.4817s,  Merge clusters: 1.4437s\n",
      "Merged clusters 59 and 28, 83 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0020s,  Get pairs: 1750758846.9264s,  Compute angles: 1.1066s,  Compute merged residuals: 1750758852.8837s,  Find candidates: 1.1408s,  Find best pair: 1750758852.8838s,  Merge clusters: 1.1425s\n",
      "Merged clusters 55 and 50, 82 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0001s,  Get pairs: 1750758854.0273s,  Compute angles: 1.1189s,  Compute merged residuals: 1750758860.2580s,  Find candidates: 1.1580s,  Find best pair: 1750758860.2581s,  Merge clusters: 1.1598s\n",
      "Merged clusters 37 and 1, 81 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0016s,  Get pairs: 1750758861.4189s,  Compute angles: 1.2650s,  Compute merged residuals: 1750758867.3276s,  Find candidates: 1.2963s,  Find best pair: 1750758867.3278s,  Merge clusters: 1.2984s\n",
      "Merged clusters 54 and 22, 80 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0005s,  Get pairs: 1750758868.6272s,  Compute angles: 1.3524s,  Compute merged residuals: 1750758874.7037s,  Find candidates: 1.3938s,  Find best pair: 1750758874.7038s,  Merge clusters: 1.3954s\n",
      "Merged clusters 47 and 29, 79 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0004s,  Get pairs: 1750758876.1002s,  Compute angles: 1.3427s,  Compute merged residuals: 1750758882.0870s,  Find candidates: 1.3800s,  Find best pair: 1750758882.0871s,  Merge clusters: 1.3815s\n",
      "Merged clusters 42 and 8, 78 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0012s,  Get pairs: 1750758883.4701s,  Compute angles: 1.1802s,  Compute merged residuals: 1750758889.1364s,  Find candidates: 1.2093s,  Find best pair: 1750758889.1366s,  Merge clusters: 1.2111s\n",
      "Merged clusters 26 and 15, 77 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0012s,  Get pairs: 1750758890.3492s,  Compute angles: 1.2207s,  Compute merged residuals: 1750758895.0797s,  Find candidates: 1.2565s,  Find best pair: 1750758895.0798s,  Merge clusters: 1.2583s\n",
      "Merged clusters 27 and 2, 76 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0003s,  Get pairs: 1750758896.3391s,  Compute angles: 1.2080s,  Compute merged residuals: 1750758901.7419s,  Find candidates: 1.2438s,  Find best pair: 1750758901.7422s,  Merge clusters: 1.2461s\n",
      "Merged clusters 72 and 23, 75 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0003s,  Get pairs: 1750758902.9893s,  Compute angles: 1.0896s,  Compute merged residuals: 1750758908.6465s,  Find candidates: 1.1466s,  Find best pair: 1750758908.6466s,  Merge clusters: 1.1488s\n",
      "Merged clusters 47 and 10, 74 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0010s,  Get pairs: 1750758909.7969s,  Compute angles: 1.0847s,  Compute merged residuals: 1750758914.4501s,  Find candidates: 1.1100s,  Find best pair: 1750758914.4502s,  Merge clusters: 1.1114s\n",
      "Merged clusters 67 and 9, 73 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0003s,  Get pairs: 1750758915.5624s,  Compute angles: 1.2164s,  Compute merged residuals: 1750758920.4966s,  Find candidates: 1.2447s,  Find best pair: 1750758920.4968s,  Merge clusters: 1.2462s\n",
      "Merged clusters 30 and 8, 72 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0004s,  Get pairs: 1750758921.7442s,  Compute angles: 0.7018s,  Compute merged residuals: 1750758926.6883s,  Find candidates: 0.7276s,  Find best pair: 1750758926.6885s,  Merge clusters: 0.7317s\n",
      "Merged clusters 38 and 35, 71 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0004s,  Get pairs: 1750758927.4211s,  Compute angles: 1.0290s,  Compute merged residuals: 1750758932.1273s,  Find candidates: 1.0604s,  Find best pair: 1750758932.1297s,  Merge clusters: 1.0683s\n",
      "Merged clusters 29 and 6, 70 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0006s,  Get pairs: 1750758933.1994s,  Compute angles: 1.1702s,  Compute merged residuals: 1750758937.0428s,  Find candidates: 1.1930s,  Find best pair: 1750758937.0430s,  Merge clusters: 1.1946s\n",
      "Merged clusters 29 and 17, 69 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0003s,  Get pairs: 1750758938.2384s,  Compute angles: 0.8396s,  Compute merged residuals: 1750758942.4169s,  Find candidates: 0.8671s,  Find best pair: 1750758942.4175s,  Merge clusters: 0.8710s\n",
      "Merged clusters 39 and 18, 68 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0003s,  Get pairs: 1750758943.2902s,  Compute angles: 1.1666s,  Compute merged residuals: 1750758947.6933s,  Find candidates: 1.1987s,  Find best pair: 1750758947.6956s,  Merge clusters: 1.2065s\n",
      "Merged clusters 37 and 4, 67 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0004s,  Get pairs: 1750758948.9038s,  Compute angles: 0.8540s,  Compute merged residuals: 1750758953.9707s,  Find candidates: 0.8882s,  Find best pair: 1750758953.9708s,  Merge clusters: 0.8898s\n",
      "Merged clusters 31 and 17, 66 clusters remain.\n",
      "Time taken for each step:\n",
      " Recompute centroids: 0.0002s,  Get pairs: 1750758954.8620s,  Compute angles: 1.0834s,  Compute merged residuals: 1750758958.6084s,  Find candidates: 1.1093s,  Find best pair: 1750758958.6085s,  Merge clusters: 1.1106s\n",
      "Merged clusters 20 and 16, 65 clusters remain.\n",
      "No more candidate pairs below threshold. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# iterative merging until no more pairs are below threshold \n",
    "\n",
    "while True:\n",
    "    t0 = time.time()\n",
    "    # 1. Recompute centroids from current clusters\n",
    "    centroids = torch.stack([clusters[i]['centroid'] for i in range(len(clusters))])\n",
    "    t1 = time.time() - t0\n",
    "    # 2. Get all pairs and their distances\n",
    "    sorted_pairs, sorted_distances = distance_pairs(centroids)\n",
    "    t2 = time.time() - t1\n",
    "    # 3. Compute angles for all pairs\n",
    "    angles = [\n",
    "        pairs_angle(clusters[i.item()], clusters[j.item()])\n",
    "        for i, j in sorted_pairs\n",
    "    ]\n",
    "    t3 = time.time() - t2\n",
    "    angles = torch.tensor(angles, device=device)\n",
    "    # 4. Compute merged residuals for all pairs\n",
    "    merged_resid = [\n",
    "        merged_residual_diff_percent(clusters[i.item()], clusters[j.item()])\n",
    "        for i, j in sorted_pairs\n",
    "    ]\n",
    "    t4 = time.time() - t3\n",
    "    merged_resid = torch.tensor(merged_resid, device=device)\n",
    "    # 5. Find candidate pairs below threshold\n",
    "    candidate_pairs_idx = merged_resid < t\n",
    "    if not torch.any(candidate_pairs_idx):\n",
    "        print(\"No more candidate pairs below threshold. Stopping.\")\n",
    "        break\n",
    "    candidate_pairs = sorted_pairs[candidate_pairs_idx]\n",
    "    merged_resid_candidates = merged_resid[candidate_pairs_idx]\n",
    "    t5 = time.time() - t4\n",
    "    # 6. Find best pair to merge\n",
    "    merging_candidate_idx = merged_resid_candidates.argmin()\n",
    "    merging_candidate_pairs = candidate_pairs[merging_candidate_idx]\n",
    "    t6 = time.time() - t5\n",
    "    idx1, idx2 = merging_candidate_pairs.tolist()\n",
    "    # 7. Merge clusters and reindex\n",
    "    clusters = update_clusters(clusters, idx1, idx2)\n",
    "    t7 = time.time() - t6\n",
    "    # 8. Print progress\n",
    "    print(\n",
    "        f\"Time taken for each step:\\n\",\n",
    "        f\"Recompute centroids: {t1:.4f}s, \",\n",
    "        f\"Get pairs: {t2:.4f}s, \",\n",
    "        f\"Compute angles: {t3:.4f}s, \",\n",
    "        f\"Compute merged residuals: {t4:.4f}s, \",\n",
    "        f\"Find candidates: {t5:.4f}s, \",\n",
    "        f\"Find best pair: {t6:.4f}s, \",\n",
    "        f\"Merge clusters: {t7:.4f}s\" \n",
    "    )\n",
    "    print(f\"Merged clusters {idx1} and {idx2}, {len(clusters)} clusters remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create point cloud from clusters \n",
    "points = [] \n",
    "cluster_idx = [] \n",
    "\n",
    "for idx, v in clusters.items():\n",
    "    points.append(clusters[idx]['points'].cpu().numpy())\n",
    "    cluster_idx.extend([idx] * v['points'].shape[0]) \n",
    "\n",
    "points = np.concatenate(points, axis=0)\n",
    "cluster_idx = np.array(cluster_idx)\n",
    "\n",
    "pc = np.hstack((points, cluster_idx[:, None]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ecd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get point cloud from clusters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7fd2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e64df290",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('sample_cluster.txt', pc, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bf46871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 16it [00:00, 71.17it/s, center_shift=0.000071, iteration=16, tol=0.000100]\n"
     ]
    }
   ],
   "source": [
    "# cluster into 120 clusters with kmeans \n",
    "clusters = initialize_clusters(sample, device, k=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "40bb1d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [] \n",
    "cluster_idx = [] \n",
    "\n",
    "for idx, v in clusters.items():\n",
    "    points.append(clusters[idx]['points'].cpu().numpy())\n",
    "    cluster_idx.extend([idx] * v['points'].shape[0]) \n",
    "\n",
    "points = np.concatenate(points, axis=0)\n",
    "cluster_idx = np.array(cluster_idx)\n",
    "\n",
    "pc = np.hstack((points, cluster_idx[:, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a9a5c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('sample_cluster.txt', pc, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_knn_patches(xyz, mask_ratio=0.7, nsample=32, random=True):\n",
    "    B, N, C = xyz.shape\n",
    "    npoint = N // nsample\n",
    "    device = xyz.device\n",
    "\n",
    "    num_patches = npoint\n",
    "    num_mask = int(mask_ratio * num_patches)\n",
    "    num_vis = num_patches - num_mask\n",
    "\n",
    "    center_idx = pointnet2_utils.furthest_point_sample(xyz, npoint).long()\n",
    "    center_pos = index_points(xyz, center_idx)\n",
    "\n",
    "    if random:\n",
    "        shuffle_idx = torch.rand(num_patches, device=device).argsort()\n",
    "        vis_patch_idx, mask_patch_idx = shuffle_idx[:num_vis], shuffle_idx[num_vis:]\n",
    "\n",
    "        mask_center_idx = center_idx[:, mask_patch_idx]\n",
    "        vis_center_idx = center_idx[:, vis_patch_idx]\n",
    "        mask_center_pos = index_points(xyz, mask_center_idx)\n",
    "        vis_center_pos = index_points(xyz, vis_center_idx)\n",
    "\n",
    "        all_patch_idx = knn_point(nsample, xyz, center_pos)  # [B, num_patches, nsample]\n",
    "        mask_idx = all_patch_idx[:, mask_patch_idx]\n",
    "        vis_idx = all_patch_idx[:, vis_patch_idx]\n",
    "        mask_pos = index_points(xyz, mask_idx)  # [B, num_mask, nsample, C]\n",
    "        vis_pos = index_points(xyz, vis_idx)  # [B, num_vis, nsample, C]\n",
    "    else:\n",
    "        mask_point_idx = np.random.randint(num_patches)\n",
    "        mask_point_pos = center_pos[:, mask_point_idx]\n",
    "        mask_patch_idx = knn_point(num_mask, center_pos, mask_point_pos.unsqueeze(1)).squeeze()\n",
    "        vis_patch_idx = torch.empty((B, num_vis), device=device, dtype=int)\n",
    "        for b in range(B):\n",
    "            idx_all = set(np.arange(num_patches, dtype=int))\n",
    "            mask_idx = set(mask_patch_idx[b].tolist())\n",
    "            vis_idx = idx_all - mask_idx\n",
    "            vis_patch_idx[b] = torch.tensor(list(vis_idx), device=device, dtype=torch.long)\n",
    "\n",
    "        shuffle_idx = torch.cat((vis_patch_idx, mask_patch_idx), dim=1).to(device)\n",
    "\n",
    "        batch_idx = torch.arange(B, device=device).unsqueeze(-1)\n",
    "        mask_center_idx = center_idx[batch_idx, mask_patch_idx]\n",
    "        vis_center_idx = center_idx[batch_idx, vis_patch_idx]\n",
    "        mask_center_pos = index_points(xyz, mask_center_idx)\n",
    "        vis_center_pos = index_points(xyz, vis_center_idx)\n",
    "\n",
    "        all_patch_idx = knn_point(nsample, xyz, center_pos)  # [B, num_patches, nsample]\n",
    "        mask_idx = all_patch_idx[batch_idx, mask_patch_idx]\n",
    "        vis_idx = all_patch_idx[batch_idx, vis_patch_idx]\n",
    "        mask_pos = index_points(xyz, mask_idx)  # [B, num_mask, nsample, C]\n",
    "        vis_pos = index_points(xyz, vis_idx)  # [B, num_vis, nsample, C]\n",
    "\n",
    "    return mask_pos,  vis_pos,  mask_center_pos, vis_center_pos, mask_patch_idx, vis_patch_idx, shuffle_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc92e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_kmeans_patches(batch, mask_ratio=0.7, nsample=32):\n",
    "    B, N, C = batch.shape \n",
    "\n",
    "    # calculate number of patches\n",
    "    num_patches = N // nsample # assuming N is num_points\n",
    "    num_masked_patches = int(num_patches * mask_ratio) \n",
    "    num_vis_patches = num_patches - num_masked_patches\n",
    "\n",
    "    vis_pos_batch = []\n",
    "    mask_pos_batch = [] \n",
    "    mask_center_pos_batch = [] \n",
    "    vis_center_pos_batch = [] \n",
    "    mask_patch_idx_batch = []\n",
    "    vis_patch_idx_batch = []\n",
    "    # generate patch_idx and shuffle_idx later \n",
    "\n",
    "    for i in range(B): \n",
    "        pc = batch[i]\n",
    "\n",
    "        model_kmeans = KMeans(num_clusters=num_patches) \n",
    "        results = model_kmeans()\n",
    "\n",
    "        cl_idx = results.labels  # [N,]\n",
    "        cl_centroids = results.centers  # [num_patches, C]\n",
    "\n",
    "        clusters = get_clusters(pc, cl_idx)\n",
    "\n",
    "        for i, cluster in enumerate(clusters): \n",
    "            if cluster.shape[0] < nsample:\n",
    "                # Randomly select indices (with replacement) to pad the cluster to nsample points\n",
    "                to_add = nsample - cluster.shape[0]\n",
    "                rand_idx = torch.randint(0, cluster.shape[0], (to_add,), device=cluster.device)\n",
    "                cluster = torch.cat([cluster, cluster[rand_idx]], dim=0)\n",
    "            elif cluster.shape[0] > nsample:\n",
    "                # If the cluster has more points than nsample, perform fps \n",
    "                idx = furthest_point_sample(\n",
    "                    cluster.unsqueeze(0), nsample\n",
    "                ).squeeze(0)\n",
    "                cluster = cluster[idx]\n",
    "            \n",
    "            \n",
    "            clusters[i] = cluster.unsqueeze(0)\n",
    "\n",
    "        clusters = torch.cat(clusters, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56adf6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_kmeans_patches(batch, patch_size=16, mask_ratio=0.2):\n",
    "    B, N, C = batch.shape \n",
    "\n",
    "    # calculate number of patches\n",
    "    num_patches = N // patch_size # assuming N is num_points\n",
    "    num_masked_patches = int(num_patches * mask_ratio) \n",
    "    num_vis_patches = num_patches - num_masked_patches\n",
    "\n",
    "    model_kmeans = KMeans(n_clusters=num_patches)\n",
    "    results = model_kmeans(batch)\n",
    "\n",
    "    cl_idx = results.labels  # [N,]\n",
    "    cl_centroids = results.centers  # [num_patches, C]\n",
    "\n",
    "    return cl_idx, cl_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c64e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        clusters = get_clusters(pc, cl_idx)\n",
    "\n",
    "        for i, cluster in enumerate(clusters): \n",
    "            if cluster.shape[0] < nsample:\n",
    "                # Randomly select indices (with replacement) to pad the cluster to nsample points\n",
    "                to_add = nsample - cluster.shape[0]\n",
    "                rand_idx = torch.randint(0, cluster.shape[0], (to_add,), device=cluster.device)\n",
    "                cluster = torch.cat([cluster, cluster[rand_idx]], dim=0)\n",
    "            elif cluster.shape[0] > nsample:\n",
    "                # If the cluster has more points than nsample, perform fps \n",
    "                idx = furthest_point_sample(\n",
    "                    cluster.unsqueeze(0), nsample\n",
    "                ).squeeze(0)\n",
    "                cluster = cluster[idx]\n",
    "            \n",
    "            \n",
    "            clusters[i] = cluster.unsqueeze(0)\n",
    "\n",
    "        clusters = torch.cat(clusters, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # get random clusters \n",
    "        shuffle_idx = torch.rand(num_patches, device=device).argsort()\n",
    "        vis_patch_idx, mask_patch_idx = shuffle_idx[:num_vis_patches], shuffle_idx[num_vis_patches:]\n",
    "\n",
    "        mask_pos = clusters[mask_patch_idx]\n",
    "        vis_pos = clusters[vis_patch_idx]\n",
    "\n",
    "        # get center points \n",
    "        mask_center_pos = torch.mean(mask_pos, dim=1)\n",
    "        vis_center_pos = torch.mean(vis_pos, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d33a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        # add to batch lists \n",
    "        vis_pos_batch.append(vis_pos)\n",
    "        mask_pos_batch.append(mask_pos)\n",
    "        mask_center_pos_batch.append(mask_center_pos)\n",
    "        vis_center_pos_batch.append(vis_center_pos)\n",
    "        mask_patch_idx_batch.append(mask_patch_idx)\n",
    "        vis_patch_idx_batch.append(vis_patch_idx)\n",
    "\n",
    "    mask_pos = torch.stack(mask_pos_batch, dim=0)  # [B, num_mask, nsample, C]\n",
    "    vis_pos = torch.stack(vis_pos_batch, dim=0)  # [B, num_vis, nsample, C]\n",
    "    mask_center_pos = torch.stack(mask_center_pos_batch, dim=0)  # [B, num_mask, C]\n",
    "    vis_center_pos = torch.stack(vis_center_pos_batch, dim=0)  # [B, num_vis, C]\n",
    "    mask_patch_idx = torch.stack(mask_patch_idx, dim=0)  # [B, num_mask]\n",
    "    vis_patch_idx = torch.stack(vis_patch_idx, dim=0)  # [B, num_vis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1a798c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b38d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full batch converged at iteration 54/100 with center shifts = tensor([0., 0., 0., 0.], device='cuda:0').\n"
     ]
    }
   ],
   "source": [
    "knn_model = KMeans(num_clusters=110) \n",
    "results = knn_model(x.unsqueeze(0))\n",
    "cl_idx = results.labels  # [N,]\n",
    "cl_centroids = results.centers  # [num_patches, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0171fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # stack batch results \n",
    "    mask_pos = torch.stack(mask_pos_batch, dim=0)  # [B, num_mask, nsample, C]\n",
    "    vis_pos = torch.stack(vis_pos_batch, dim=0)  # [B, num_vis, nsample, C]\n",
    "    mask_center_pos = torch.stack(mask_center_pos_batch, dim=0)  # [B, num_mask, C]\n",
    "    vis_center_pos = torch.stack(vis_center_pos_batch, dim=0)  # [B, num_vis, C]\n",
    "    mask_patch_idx = torch.stack(mask_patch_idx, dim=0)  # [B, num_mask]\n",
    "    vis_patch_idx = torch.stack(vis_patch_idx, dim=0)  # [B, num_vis]\n",
    "\n",
    "    # create shuffle index\n",
    "    shuffle_idx = torch.cat((vis_patch_idx, mask_patch_idx), dim=1)\n",
    "\n",
    "    return mask_pos, vis_pos, mask_center_pos, vis_center_pos, mask_patch_idx, vis_patch_idx, shuffle_idx\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c954af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds, \n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d781a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cc443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch = batch[0].to(device)  # move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e91a19b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2048, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5925878",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 55.62 MiB is free. Process 2324173 has 10.49 GiB memory in use. Including non-PyTorch memory, this process has 210.00 MiB memory in use. Of the allocated memory 8.47 MiB is allocated by PyTorch, and 13.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clusters \u001b[38;5;241m=\u001b[39m \u001b[43msplit_kmeans_patches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 10\u001b[0m, in \u001b[0;36msplit_kmeans_patches\u001b[0;34m(batch, patch_size, mask_ratio)\u001b[0m\n\u001b[1;32m      7\u001b[0m num_vis_patches \u001b[38;5;241m=\u001b[39m num_patches \u001b[38;5;241m-\u001b[39m num_masked_patches\n\u001b[1;32m      9\u001b[0m model_kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mnum_patches)\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_kmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m cl_idx \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mlabels  \u001b[38;5;66;03m# [N,]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m cl_centroids \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mcenters  \u001b[38;5;66;03m# [num_patches, C]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/mae3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mae3d/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/mae3d/lib/python3.10/site-packages/torch_kmeans/clustering/kmeans.py:277\u001b[0m, in \u001b[0;36mKMeans.forward\u001b[0;34m(self, x, k, centers, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     centers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_center_init(x, k, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    273\u001b[0m centers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_centers(\n\u001b[1;32m    274\u001b[0m     centers, dims\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    275\u001b[0m )\n\u001b[0;32m--> 277\u001b[0m labels, new_centers, inertia, soft_assign \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cluster\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ClusterResult(\n\u001b[1;32m    281\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     centers\u001b[38;5;241m=\u001b[39mnew_centers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     soft_assignment\u001b[38;5;241m=\u001b[39msoft_assign,\n\u001b[1;32m    288\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/mae3d/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/mae3d/lib/python3.10/site-packages/torch_kmeans/clustering/kmeans.py:542\u001b[0m, in \u001b[0;36mKMeans._cluster\u001b[0;34m(self, x, centers, k, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m old_centers \u001b[38;5;241m=\u001b[39m centers\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# get cluster assignments\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m c_assign \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# update cluster centers\u001b[39;00m\n\u001b[1;32m    544\u001b[0m centers \u001b[38;5;241m=\u001b[39m group_by_label_mean(x, c_assign, k_max_range)\n",
      "File \u001b[0;32m~/.conda/envs/mae3d/lib/python3.10/site-packages/torch_kmeans/clustering/kmeans.py:590\u001b[0m, in \u001b[0;36mKMeans._assign\u001b[0;34m(self, x, centers, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Infer cluster assignment for each sample in x.\"\"\"\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# dist: (bs, num_init, n, k_max)\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pairwise_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance, (CosineSimilarity, DotProductSimilarity)):\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# Similarity is an inverted distance measure,\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# so we need to adapt it in order to calculate priority\u001b[39;00m\n\u001b[1;32m    594\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dist\n",
      "File \u001b[0;32m~/.conda/envs/mae3d/lib/python3.10/site-packages/torch_kmeans/clustering/kmeans.py:579\u001b[0m, in \u001b[0;36mKMeans._pairwise_distance\u001b[0;34m(self, x, centers, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m bs, n, d \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    578\u001b[0m bs, num_init, k_max, d \u001b[38;5;241m=\u001b[39m centers\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 579\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m centers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    581\u001b[0m     centers[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mexpand(bs, num_init, n, k_max, d)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, d)\n\u001b[1;32m    582\u001b[0m )\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mpairwise_distance(x, centers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m    584\u001b[0m     bs, num_init, n, k_max\n\u001b[1;32m    585\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 55.62 MiB is free. Process 2324173 has 10.49 GiB memory in use. Including non-PyTorch memory, this process has 210.00 MiB memory in use. Of the allocated memory 8.47 MiB is allocated by PyTorch, and 13.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "clusters = split_kmeans_patches(batch, patch_size=16, mask_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2472fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2048]), torch.Size([4, 8, 3]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[0].shape, clusters[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d25cfcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2048]), torch.Size([4, 8, 3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[0].shape, clusters[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "680f73e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2048]), torch.Size([4, 8, 3]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters[0].shape, clusters[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "06f7090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89, 16, 3])\n",
      "torch.Size([39, 16, 3])\n",
      "torch.Size([89, 3])\n",
      "torch.Size([39, 3])\n",
      "torch.Size([89])\n",
      "torch.Size([39])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for i in clusters: \n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "cd85feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = furthest_point_sample(cluster.unsqueeze(0), 16).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "88abb7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = cluster[idx]\n",
    "cluster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a52d1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mae3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
