experiment_setup:
  name: baseline_cu_patching_als50k
  experiment_dir: null # set automatically
  checkpoints_dir: null
  visualization_dir: null
  logs_dir: null
  results_dir: null
  seed: 42 
  use_cuda: true
  deterministic: true
  visualize: true # last batch of last epoch will be visualized

model:
  patch_size: 16  # number of points per patch 
  mask_ratio: 0.3
  num_points: 2048 # number points per instance
  masking_strategy: xysplit # can be 'random', 'block', 'xysplit'
  encoder_dims: 1024 
  decoder_dims: 1024
  mlp_dim: 2048
  depth: 6
  num_heads: 8 
  dim_per_head: 64 
  dropout: 0.5
  loss_alpha_center: 0.1


pretraining:
  epochs: 100
  batch_size: 32
  num_workers: 8
  shuffle: true
  pin_memory: true
  prefetch_factor: 2

  save_interval: 30
  resume: false 
  resume_path: null
  
  
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001
    weight_decay: 0.0001

  scheduler: 
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR 
    T_max: 200
    eta_min: 0.0001

  dataset:
    name: ALS_50K

downstream: 
  pretrained: true 
  task: regression # can be regression or classification for FORage
  train_type: finetune # can be probing, finetune, or scratch
  repetitions: 3
  resume: false 
  resume_path: null

  epochs: 300
  batch_size: 16
  num_workers: 8
  shuffle: true
  pin_memory: true
  prefetch_factor: 2
  clip_grad_norm: null #1.0 # gradient clipping, set to null to disable
  clip_grad_value: 1
  #save_interval: 25 

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001
    weight_decay: 0.01

  schedulers: # list of schedulers as they are applied sequentially: pos matters
  - _target_: torch.optim.lr_scheduler.LinearLR
    start_factor: 0.1 
    end_factor: 1.0
    total_iters: 30
  - _target_: torch.optim.lr_scheduler.CosineAnnealingLR 
    T_max: 270
    eta_min: 0.000001

  dataset:
    name: FORAGE 
    fraction: 1.0
  
  regression_criterion:
    _target_: torch.nn.SmoothL1Loss


   




# hydra configuration - no need to change 
hydra:
  run:
    dir: experiments/${experiment_setup.name}
